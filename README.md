# AI Chatbot - Powered by Ollama

A modern AI chatbot application built with Next.js and powered by Ollama for local AI inference.

## Prerequisites

Before running this application, make sure you have [Ollama](https://ollama.ai/) installed and running locally:

1. Install Ollama from [https://ollama.ai/](https://ollama.ai/)
2. Pull a model (e.g., `ollama pull llama3.2`)
3. Start Ollama service (usually runs on `http://localhost:11434`)

## Getting Started

First, install the dependencies:

```bash
npm install
# or
yarn install
# or
pnpm install
```

Then, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to start chatting with AI.

## Features

- ğŸ¤– **Local AI Chat** powered by Ollama
- âš¡ **Next.js 14** with App Router
- ğŸ’… **Styled Components** for modern styling
- ğŸ“ **TypeScript** for type safety
- ğŸ§¹ **ESLint** for code linting
- ğŸ¨ **Beautiful UI** with glassmorphism design
- ğŸ“± **Responsive design** for all devices
- ğŸ”„ **Real-time streaming** chat interface
- ğŸ§  **Conversation context** maintained throughout the session
- ğŸ—‘ï¸ **Clear conversation** to start fresh
- âš¡ **Streaming responses** with Server-Sent Events (SSE)
- ğŸ’­ **Live typing indicators** during AI response generation
- ğŸ§  **Thinking process display** for reasoning models (DeepSeek-R1)
- ğŸ” **Expandable thinking sections** to see AI's reasoning steps
- ğŸ“ **LaTeX math rendering** with KaTeX (supports `\[...\]` and `$...$`)
- ğŸ§® **Real-time math display** during streaming responses
- ğŸš€ **Fast local inference** with progressive text display

## Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ chat/
â”‚   â”‚       â””â”€â”€ stream/
â”‚   â”‚           â””â”€â”€ route.ts # Streaming API endpoint
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ StyledComponentsRegistry.tsx  # Styled components setup
â”‚   â”œâ”€â”€ globals.css          # Global styles
â”‚   â”œâ”€â”€ layout.tsx           # Root layout
â”‚   â””â”€â”€ page.tsx             # Chat interface
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ ollama.ts            # Ollama client utilities (with streaming)
â”œâ”€â”€ .eslintrc.json          # ESLint configuration
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ next.config.js          # Next.js configuration
â”œâ”€â”€ package.json            # Dependencies and scripts
â”œâ”€â”€ README.md               # This file
â””â”€â”€ tsconfig.json           # TypeScript configuration
```

## Configuration

You can modify the AI model in `lib/ollama.ts`:

```typescript
// Change this to any model you have installed via Ollama
model: 'deepseek-r1:latest'  // or 'llama3.2', 'llama2', 'codellama', etc.
```

### DeepSeek-R1 Reasoning Display

This chatbot has special support for **DeepSeek-R1** and other reasoning models that include thinking processes:

- **Thinking Tags**: Content between `<think>` and `</think>` tags is parsed as reasoning
- **Separate Display**: Thinking content is shown in a styled, collapsible section
- **State Machine**: Handles split tags across streaming chunks correctly
- **Toggle View**: Users can expand/collapse thinking sections per message

### LaTeX Math Rendering

Full support for mathematical expressions with **KaTeX**:

- **Display Math**: `\[equation\]` renders as centered block equations
- **Inline Math**: `$equation$` renders inline within text
- **Real-time Rendering**: Math appears as it streams from the AI
- **Mixed Content**: Text, thinking, and math all rendered together
- **Error Handling**: Graceful fallback for invalid LaTeX syntax

**Example formats supported**:
```latex
\[S_{n+1} = \frac{n(n+1)}{2} + \frac{2(n+1)}{2}\]  # Display math
The equation $E = mc^2$ is famous.                    # Inline math
```

## Architecture

This application uses **streaming responses** for real-time chat experience:

- **Streaming API**: `/api/chat/stream` endpoint with Server-Sent Events (SSE)
- **Progressive Display**: Text appears as it's generated by Ollama
- **Live Indicators**: Visual feedback during response generation
- **Context Preservation**: Full conversation history maintained
- **Error Handling**: Graceful fallbacks for connection issues

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details. 